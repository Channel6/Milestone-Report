---
title: "Coursera Capstone Milestone Report"
author: "Glenn Kerbein"
date: "July 30, 2017"
output:
  html_document:
    theme: cerulean
    toc: true 
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

# Report Details

The goal of this project is just to display that youâ€™ve gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on [R Pubs](http://rpubs.com/) that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set.

The motivation for this project is to:

1. Demonstrate that you've downloaded the data and have successfully loaded it in.
2. Create a basic report of summary statistics about the data sets.
3. Report any interesting findings that you amassed so far.
4. Get feedback on your plans for creating a prediction algorithm and Shiny app.

# Demonstrate successful data fetch and load

Download the SwiftKey dataset. If it does not exist, then download and unzip it.
```{r echo=TRUE}
SwiftKey_data_url = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
SwiftKey_destination_zip = "Coursera-SwiftKey.zip"

if (!file.exists("./data/final")) {
  if (!file.exists(paste0("./data/", SwiftKey_destination_zip))) {
    download.file(
      SwiftKey_data_url,
      destfile = SwiftKey_destination_zip,
      quiet=TRUE
    )
  }
  
  unzip("Coursera-SwiftKey.zip", exdir = "./data");
}
```
Load necessary files.
```{r echo=FALSE}
# Preload necessary R librabires
library(dplyr)
library(doParallel)
library(stringi)
library(ggplot2)

# Setup parallel clusters to accelarate execution time
jobcluster <- makeCluster(detectCores())
invisible(clusterEvalQ(jobcluster, library(stringi)))
```

According to the specifications, load the English data set only.
```{r echo=TRUE}
blogs <- readLines("./data/final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul=TRUE)
tweets <-  readLines("./data/final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul=TRUE)

# News dataset looks like a raw binary file to R. To mitigate this, open the file as a stream,
# then read the data, line by line, into a new variable.
con <- file("./data/final/en_US/en_US.news.txt", open="rb")
news <- readLines(con, encoding="UTF-8", skipNul=TRUE)
close(con)
rm(con)
```

Grab sample data.

```{r echo=TRUE}
head(blogs, 10)
```
```{r echo=TRUE}
head(tweets, 5)
```
```{r echo=TRUE}
head(news, 5)
```

# Basic report of summary statistics about the data sets

We analyse basic statistics of the 3 data files, including **line**, **character** and **word** counts, and Words Per Line (WPL) summaries. Basic histograms are also plotted to identified distribution of these data.

```{r echo=TRUE}
rawWPL<-lapply(
		list(blogs,news,tweets),
		function(x) stri_count_words(x)
	)

rawstats<-data.frame(
	File=c("blogs","news","tweets"),
	t(
		rbind(
			sapply(
				list(blogs,news,tweets),
				stri_stats_general
			),
			TotalWords=sapply(
				list(blogs,news,tweets),
				stri_stats_latex
			)[4,]
		)
	),
	# Compute words per line summary
	WPL=rbind(
		summary(rawWPL[[1]]),
		summary(rawWPL[[2]]),
		summary(rawWPL[[3]])
	)
)
print(rawstats)
```
```{r echo=TRUE}
qplot(
	rawWPL[[1]],
	geom="histogram",
	main="Histogram for US Blogs",
	xlab="No. of Words",
	ylab="Frequency",
	binwidth=10
)
```
```{r echo=TRUE}
qplot(
	rawWPL[[2]],
	geom="histogram",
	main="Histogram for US News",
	xlab="No. of Words",
	ylab="Frequency",
	binwidth=10
)
```
```{r echo=TRUE}
qplot(
	rawWPL[[3]],
	geom="histogram",
	main="Histogram for US Tweets",
	xlab="No. of Words",
	ylab="Frequency",
	binwidth=1
)
```
```{r echo=TRUE}
rm(rawWPL);rm(rawstats)
```

# Report any interesting findings that you amassed so far

The frequency distributions of the "blogs" and "news" corpora are similar (appearing to be log-normal).
The frequency distribution of the "tweets" corpus is again different, as a result of the 140 character limit.

From the statistics, we observed that WPL for blogs are generally higher (at 41.75 mean), followed by news (at 34.41 mean) and tweets (at 12.75 mean). This may be reflective of the expected attention-span of readers of these contents.

From the histograms, we also noticed that the WPL for all data types are right-skewed (i.e. longer right tail). This may be an indication of the general trend towards short and concised communications.

# Get feedback on your plans for creating a prediction algorithm and Shiny app

## A big-picture approach to accomplishing the Shiny application

1. Continuing cleaning the corpus to to increase the accuracy of the model;
2. Define and refine the sampling process for getting a good ngram representation without using the entire corpus;
3. Build and test final prediction model.

Current word prediction will employ mono-gram prediction data. Next-wor prediction will employ: bi-gram, tri-gram and quadri-gram prediction data.

## Requirements for the final project

Work on the training predictive models using training data sets within the corpora.
Compare respective models for each type of text - blogs, news & Twitter & to understand and how these perform against aggregate models trained off the entire corpus that spans the 3 types of text files.

## External resources
Hat tip [Eric Lim B G](https://rpubs.com/EricLimBG/capstone-milestone "Capstone Project Milestone Report") for reading news as binary.

